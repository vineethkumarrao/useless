#!/usr/bin/env python3
"""
Test Script for LLM Model API Integration
Tests Google's Gemini model via LangChain ChatGoogleGenerativeAI
"""

import os
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, SystemMessage

# Load environment variables
load_dotenv()

def test_basic_llm_invoke():
    """Test basic LLM invoke with simple prompt"""
    print("üß™ Testing Basic LLM Invoke")
    
    try:
        # Initialize LLM
        llm = ChatGoogleGenerativeAI(
            model="gemini-1.5-flash",
            temperature=0.3,
            max_tokens=512,
            google_api_key=os.getenv('GOOGLE_API_KEY')
        )
        
        # Simple prompt test
        prompt = "Explain what Python is in 2-3 sentences."
        response = llm.invoke(prompt)
        
        print(f"‚úÖ Basic Response: {response.content[:200]}...")
        print(f"üìä Tokens: {len(response.content.split())} words")
        
    except Exception as e:
        print(f"‚ùå Basic Invoke Error: {e}")

def test_message_structure():
    """Test LLM with proper message structure"""
    print("\nüß™ Testing Message Structure")
    
    try:
        llm = ChatGoogleGenerativeAI(
            model="gemini-1.5-flash",
            temperature=0.3,
            max_tokens=512,
            google_api_key=os.getenv('GOOGLE_API_KEY')
        )
        
        # Test with HumanMessage
        messages = [
            HumanMessage(content="What are the benefits of renewable energy?"),
            SystemMessage(content="You are a helpful assistant. Keep responses concise and informative.")
        ]
        
        response = llm.invoke(messages)
        
        print(f"‚úÖ Message Response: {response.content[:200]}...")
        
        # Test conversation flow
        follow_up_messages = [
            HumanMessage(content="Can you give an example of solar power?"),
            SystemMessage(content="Provide practical examples.")
        ]
        
        follow_up_response = llm.invoke(follow_up_messages)
        print(f"‚úÖ Follow-up Response: {follow_up_response.content[:150]}...")
        
    except Exception as e:
        print(f"‚ùå Message Structure Error: {e}")

def test_error_handling():
    """Test LLM error handling scenarios"""
    print("\nüß™ Testing Error Handling")
    
    scenarios = [
        "Invalid model name test",
        "Long prompt to test token limits",
        "Empty prompt test"
    ]
    
    for i, scenario in enumerate(scenarios, 1):
        try:
            llm = ChatGoogleGenerativeAI(
                model="gemini-1.5-flash" if "invalid" not in scenario else "invalid-model",
                temperature=0.3,
                max_tokens=100,
                google_api_key=os.getenv('GOOGLE_API_KEY')
            )
            
            if "long" in scenario:
                prompt = "A" * 10000  # Very long prompt
            elif "empty" in scenario:
                prompt = ""
            else:
                prompt = "Simple test"
                
            response = llm.invoke(prompt)
            print(f"‚úÖ Scenario {i} Success: Response length {len(response.content)} chars")
            
        except Exception as e:
            print(f"‚úÖ Scenario {i} Error Caught: {str(e)[:100]}...")

def test_streaming_response():
    """Test streaming response (if supported)"""
    print("\nüß™ Testing Streaming Response")
    
    try:
        llm = ChatGoogleGenerativeAI(
            model="gemini-1.5-flash",
            temperature=0.3,
            max_tokens=300,
            google_api_key=os.getenv('GOOGLE_API_KEY'),
            streaming=True
        )
        
        prompt = "Write a short poem about artificial intelligence."
        
        print("Streaming response:")
        full_response = ""
        for chunk in llm.stream([HumanMessage(content=prompt)]):
            if hasattr(chunk, 'content') and chunk.content:
                print(chunk.content, end='', flush=True)
                full_response += chunk.content
        print("\n‚úÖ Streaming completed successfully")
        
    except Exception as e:
        print(f"‚ùå Streaming Error: {e}")

def test_api_key_validation():
    """Test API key validation"""
    print("\nüß™ Testing API Key Validation")
    
    api_key = os.getenv('GOOGLE_API_KEY')
    
    if api_key:
        print(f"‚úÖ API Key Found: {len(api_key)} characters (valid format)")
        
        # Test with actual key
        try:
            llm = ChatGoogleGenerativeAI(
                model="gemini-1.5-flash",
                temperature=0.3,
                max_tokens=100,
                google_api_key=api_key
            )
            
            # Quick health check
            response = llm.invoke("Say 'API working!'")
            if "API working" in response.content.lower():
                print("‚úÖ API Key Validation: Working correctly!")
            else:
                print("‚ö†Ô∏è API Key seems valid but response unexpected")
                
        except Exception as e:
            print(f"‚ùå API Key Error: {e}")
    else:
        print("‚ùå API Key Not Found in Environment")

if __name__ == "__main__":
    print("üöÄ Testing LLM Model API Integration")
    print("=" * 50)
    
    if not os.getenv('GOOGLE_API_KEY'):
        print("‚ùå GOOGLE_API_KEY not found! Please set it in .env file")
        print("Example: GOOGLE_API_KEY=your_actual_api_key_here")
    else:
        test_api_key_validation()
        test_basic_llm_invoke()
        test_message_structure()
        test_error_handling()
        # test_streaming_response()  # Uncomment if streaming is needed
        
    print("\n‚úÖ LLM API Testing Complete!")